{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Xl0RLJc7xlT"
      },
      "source": [
        "In this notebook, we fine-tune the OpenLLama model using QLoRA and the Dolly15k dataset. Our goal is to develop a model capable of answering questions based on provided\n",
        "context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Ll-ziXyqof6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.24.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.10.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: colorama in c:\\users\\bora\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.43.1)\n",
            "Requirement already satisfied: torch in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bitsandbytes) (2.2.2)\n",
            "Requirement already satisfied: numpy in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bitsandbytes) (1.24.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->bitsandbytes) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->bitsandbytes) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch->bitsandbytes) (2024.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\bora\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets\n",
        "%pip install -q -U git+https://github.com/lvwerra/trl.git\n",
        "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "%pip install git+https://github.com/huggingface/transformers.git -q -U # transformers version:  4.37.0\n",
        "%pip install git+https://github.com/huggingface/accelerate.git -q -U # accelerate version:  0.27.0\n",
        "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "%pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OuTqKiefqqOM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Bora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import transformers\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, LlamaTokenizer\n",
        "from trl import SFTTrainer\n",
        "from IPython.display import display, Markdown\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iN_8mR_8EO7"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hpA8j-prqsLU"
      },
      "outputs": [],
      "source": [
        "# Load the dolly-15k dataset\n",
        "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3HUypudB8HbD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'context', 'response', 'category'],\n",
              "        num_rows: 15011\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dolly_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tscUO40y8Sc1"
      },
      "source": [
        "The dataset contains instruction, context, and response triplets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZWYR-7iy8TCg"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'When did Virgin Australia start operating?',\n",
              " 'context': \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\",\n",
              " 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.',\n",
              " 'category': 'closed_qa'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dolly_dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSc8XAeJ8evI"
      },
      "source": [
        "Drop examples that are longer than 2200 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "q_4f0c7-8eBH"
      },
      "outputs": [],
      "source": [
        "def drop_long_sequences(dataset_obj):\n",
        "    \"\"\"\n",
        "    Identifies indices of entries in a dataset that exceed a certain sequence length.\n",
        "\n",
        "    Args:\n",
        "    dataset_obj (iterable): dataset where each entry is a dictionary with keys 'instruction', 'context', and 'response'.\n",
        "\n",
        "    Returns:\n",
        "    list: Indices of dataset entries ('instruction', 'context', and 'response') that are longer than 2200 characters in total.\n",
        "    \"\"\"\n",
        "\n",
        "    long_sequence_indices = []\n",
        "\n",
        "    for i, entry in enumerate(dataset_obj):\n",
        "        total_length = len(entry['instruction']) + len(entry['context']) + len(entry['response'])\n",
        "        if total_length > 2200:\n",
        "            long_sequence_indices.append(i)\n",
        "\n",
        "    return long_sequence_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "paQq1tdY8rbs"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter 'indices'=<generator object <genexpr> at 0x000001F8C5332EA0> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        }
      ],
      "source": [
        "indices_to_drop = drop_long_sequences(dolly_dataset[\"train\"])\n",
        "dolly_dataset_reduced = dolly_dataset[\"train\"].select(i for i in range(len(dolly_dataset[\"train\"])) if i not in set(indices_to_drop))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KVIuWW9q87gJ"
      },
      "outputs": [],
      "source": [
        "# Split the data into train (90%) and test (10%) sets (You can use train_test_split() function from huggingface)\n",
        "\n",
        "dataset_prepared = Dataset.train_test_split(dolly_dataset_reduced, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['instruction', 'context', 'response', 'category'],\n",
              "        num_rows: 12667\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['instruction', 'context', 'response', 'category'],\n",
              "        num_rows: 1408\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-eOfpT5-Po_"
      },
      "source": [
        "# Input Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8JrKUmv-WNP"
      },
      "source": [
        "We need to properly prepare and format the dataset before presenting it to the model. The input prompts given to the model are structured using the formatting function described below.\n",
        "\n",
        "You can come up with your own prompts. Here is a sample prompt:\n",
        "```\n",
        "      input_prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      f\"### Instruction:\\n {instruction}\\n\\n\"\n",
        "      f\"### Input:\\n {context}\\n\\n\"\n",
        "      f\"### Response:\\n {response}\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UPp_cGiWNF9F"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "   \"\"\"\n",
        "   Formats a given example dictionary into a structured text prompt based on the presence of contextual information.\n",
        "   Args:\n",
        "   example (dict): A dictionary expected to contain 'instruction', 'response', and optionally 'context'.\n",
        "   Returns:\n",
        "   dict: A dictionary with a single key 'text' that holds the formatted instruction as its value.\n",
        "   \"\"\"\n",
        "\n",
        "   # If there is a context, give \"instruction\", \"context\", and \"response\" as a prompt\n",
        "   # Else, just give \"instruction\" & \"response\" pair\n",
        "\n",
        "   if 'context' in example and example['context']:\n",
        "      formatted_text = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "                          \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "                          f\"### Instruction:\\n {example['instruction']}\\n\\n\"\n",
        "                          f\"### Input:\\n {example['context']}\\n\\n\"\n",
        "                          f\"### Response:\\n {example['response']}\")\n",
        "   else:\n",
        "      formatted_text = (\"Below is an instruction that describes a task. \"\n",
        "                         \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "                         f\"### Instruction:\\n {example['instruction']}\\n\\n\"\n",
        "                         f\"### Response:\\n {example['response']}\")\n",
        "       \n",
        "   return {'text': formatted_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F6l1h-Sv9zae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 12667/12667 [00:01<00:00, 12158.76 examples/s]\n",
            "Map: 100%|██████████| 1408/1408 [00:00<00:00, 11239.39 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Format the dataset using the function above\n",
        "formatted_dataset = dataset_prepared.map(formatting_func)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3ug39evRWTS"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1E5M5-hTb8t"
      },
      "source": [
        "We use the `openlm-research/open_llama_7b_v2` model. Alternatively, you could use the `openlm-research/open_llama_3b` model, which has fewer parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LbyPeSktRXTT"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "\n",
        "model_id = \"openlm-research/open_llama_7b_v2\"\n",
        "\n",
        "# Define a BitsAndBytesConfig object with load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, \n",
        "    bnb_4bit_use_double_quant=True, \n",
        "    bnb_4bit_quant_type=\"nf4\", \n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCRSG55VVlwB"
      },
      "outputs": [],
      "source": [
        "# Load the model & tokenizer\n",
        "\n",
        "# Load the base model \"openlm-research/open_llama_7b_v2\" using AutoModelForCausalLM.from_pretrained(). Remember to use bnb_config as the quantization_config while loading.\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"openlm-research/open_llama_7b_v2\", quantization_config=bnb_config)\n",
        "\n",
        "# Load the tokenizer of the model \"openlm-research/open_llama_7b_v2\" using LlamaTokenizer.from_pretrained() function.\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"openlm-research/open_llama_7b_v2\")\n",
        "\n",
        "# Add the padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWX6ZoXTY11a"
      },
      "source": [
        "We use Supervised Fine-tuning Trainer (`SFTTrainer`) for training. Feel free to try different values for `learning rate` and `max_steps`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmOlkoXcX3a2"
      },
      "outputs": [],
      "source": [
        "# Define a LoraConfig object with with r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\" (You can change these hyperparameters)\n",
        "qlora_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\") # YOUR CODE HERE\n",
        "\n",
        "train_dataset, eval_dataset = formatted_dataset['train'], formatted_dataset['test']\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    base_model,\n",
        "    train_dataset=train_dataset, \n",
        "    eval_dataset=eval_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4, \n",
        "        max_steps=10000, \n",
        "        output_dir=\"./OpenLLama7B-Dolly15k\", \n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        fp16=True,\n",
        "    ),\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config=qlora_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-OH5wgqaW5C"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlr_HTEXavHQ"
      },
      "outputs": [],
      "source": [
        "# Save the model using save_model()\n",
        "\n",
        "os.mkdir('./artifacts')\n",
        "trainer.save_model('./artifacts')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rW07tZ_8a9h9"
      },
      "outputs": [],
      "source": [
        "# Load the saved model & tokenizer\n",
        "\n",
        "# Load lora_config from where you saved the checkpoint\n",
        "lora_config = LoraConfig.from_pretrained(\"./artifacts\")\n",
        "\n",
        "# Specify the BitsAndBytesConfig (use load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    lora_config.base_model_name_or_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={\"\":0})\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Load the tokenizer from the checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./artifacts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y24xnolGjlTX"
      },
      "source": [
        " # Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5jURu63oHhF"
      },
      "source": [
        "Before providing the instruction and context to the model, we first prepare the prompt using the `make_inference()` function. We then tokenize these inputs and feed them to the model. The prompts prepared in this function should follow the same format as those created by the `formatting_func()`.\n",
        "\n",
        "You can come up with your own prompts. Here is a sample prompt:\n",
        "```\n",
        "      input_prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "      f\"### Instruction:\\n {instruction}\\n\\n\"\n",
        "      f\"### Input:\\n {context}\\n\\n\"\n",
        "      f\"### Response:\\n\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trhnZetPntoI"
      },
      "outputs": [],
      "source": [
        "def make_inference(instruction, context=None):\n",
        "    \"\"\"\n",
        "    Generates responses from different models based on the provided instruction and optional context.\n",
        "\n",
        "    Args:\n",
        "    instruction (str): The instruction for the task.\n",
        "    context (str, optional): Additional context for the task. Defaults to None.\n",
        "    \"\"\"\n",
        "    if context:\n",
        "        prompt = context + \" \" + instruction\n",
        "    else:\n",
        "        prompt = instruction\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda:0\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
        "    display(Markdown(tokenizer.decode(outputs[0], skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU3Y8_vgqHtM"
      },
      "source": [
        "# Sample Inferences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0n6CLKBqOEU"
      },
      "outputs": [],
      "source": [
        "make_inference(\"Identify the odd one out and explain your choice.\", \"Orange, Green, Airplane.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf104SzrqL_Q"
      },
      "outputs": [],
      "source": [
        "make_inference(\"Explain in simple terms how the attention mechanism of a transformer model works\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
