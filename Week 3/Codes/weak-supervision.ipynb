{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac9f36e-5b84-4d86-8991-8700e90bfe89",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4203937-472f-4802-a099-865c2a7fda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import urllib3\n",
    "from typing import List, Dict, Union, Tuple, Optional\n",
    "\n",
    "import faiss\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import (\n",
    "    LoggingHandler,\n",
    "    SentencesDataset,\n",
    "    SentenceTransformer,\n",
    "    losses,\n",
    "    models,\n",
    "    util,\n",
    ")\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
    "from sentence_transformers.evaluation import (\n",
    "    BinaryClassificationEvaluator,\n",
    "    EmbeddingSimilarityEvaluator,\n",
    ")\n",
    "from sentence_transformers.readers import InputExample, STSBenchmarkDataReader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1002b8-de2a-43cf-b60f-0004575d15d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    level=logging.INFO,\n",
    "    handlers=[LoggingHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32890c6-1251-4a24-b615-cea4c4af1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_PATH = \"./\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "biencoder_name = \"facebook/mcontriever\"\n",
    "crossencoder_name = \"jeffwan/mmarco-mMiniLMv2-L12-H384-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8256e9c-0b7a-4775-9bd1-df9a9989ddfd",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b6610f1-d5d8-4a21-9284-5acfe8ba25e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetProcessor:\n",
    "    @staticmethod\n",
    "    def sts_to_dataframe(data, output_file: Optional[str] = None, save: bool = False) -> pd.DataFrame:\n",
    "        out = []\n",
    "        with tqdm(total=len(data), desc=\"Normalizing Scores\") as progressbar:\n",
    "            for sample in data:\n",
    "                sentence1 = sample[\"sentence1\"]\n",
    "                sentence2 = sample[\"sentence2\"]\n",
    "                label = float(sample[\"similarity_score\"])/5.0\n",
    "                out.append([sentence1, sentence2, label])\n",
    "                progressbar.update()\n",
    "\n",
    "        df = pd.DataFrame(out, columns=[\"sentence1\", \"sentence2\", \"label\"])\n",
    "\n",
    "        if save:\n",
    "            df.to_csv(os.path.join(SESSION_PATH, name))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def qoura_to_dataframe(data, output_file: Optional[str] = None, save: bool = False) -> pd.DataFrame:\n",
    "        out = []\n",
    "        with tqdm(total=len(data)) as progressbar:\n",
    "            for sample in data:\n",
    "                sentence1 = sample[\"questions\"][\"text\"][0]\n",
    "                sentence2 = sample[\"questions\"][\"text\"][1]\n",
    "                out.append([sentence1, sentence2])\n",
    "                progressbar.update()\n",
    "                \n",
    "        df = pd.DataFrame(out, columns=[\"sentence1\", \"sentence2\"])\n",
    "\n",
    "        if save:\n",
    "            df.to_csv(os.path.join(SESSION_PATH, name))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_text(text):\n",
    "        if text.isdigit():\n",
    "            return True\n",
    "    \n",
    "        if all(char in string.punctuation for char in text):\n",
    "            return True\n",
    "    \n",
    "        if any(len(word) > 15 for word in text.split()):\n",
    "            return True\n",
    "    \n",
    "        words = text.split()\n",
    "        if all(word.lower() in STOPWORDS for word in words):\n",
    "            return len(words) > 0\n",
    "        return False\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_quora(df):\n",
    "        mask = df.apply(lambda row: DatasetProcessor.filter_text(row[\"sentence1\"]) or DatasetProcessor.filter_text(row[\"sentence2\"]), axis=1)\n",
    "        return df[~mask].reset_index(drop=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_sbert_dataset(df: Union[pd.DataFrame, str], negative_sampling=True, label=True):\n",
    "        examples = []\n",
    "        if type(df) == str:\n",
    "            data = pd.read_csv(path)\n",
    "        else:\n",
    "            data = df.copy()\n",
    "\n",
    "        with tqdm(total=len(data)) as progressbar:\n",
    "            for idx, row in data.iterrows():\n",
    "                sentence1 = row[\"sentence1\"]\n",
    "                sentence2 = row[\"sentence2\"]\n",
    "                label = row[\"label\"]\n",
    "                \n",
    "                assert type(label) == float\n",
    "                assert type(sentence1) == str\n",
    "                assert type(sentence2) == str\n",
    "                \n",
    "                inp_example = InputExample(\n",
    "                    texts=[sentence1, sentence2],\n",
    "                    label=label\n",
    "                )\n",
    "    \n",
    "                examples.append(inp_example)\n",
    "                progressbar.update()\n",
    "                \n",
    "        return examples\n",
    "\n",
    "    @staticmethod\n",
    "    def negative_sample(df: pd.DataFrame, n: int, to_df: bool = True):\n",
    "        G = nx.Graph()\n",
    "        for index, row in df.iterrows():\n",
    "            G.add_edge(row[\"sentence1\"], row[\"sentence2\"])\n",
    "\n",
    "        all_sentences = list(set(df[\"sentence1\"]).union(set(df[\"sentence2\"])))\n",
    "        negative_samples = [DatasetProcessor.generate_negative_sample(G, all_sentences) for _ in tqdm(range(n))]\n",
    "        if to_df:\n",
    "            negative_samples = pd.DataFrame(negative_samples, columns=[\"sentence1\", \"sentence2\"])\n",
    "            negative_samples[\"label\"] = 0.0\n",
    "        return negative_samples\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_negative_sample(G, all_sentences):\n",
    "        while True:\n",
    "            s1, s2 = random.sample(all_sentences, 2)\n",
    "            if not G.has_edge(s1, s2) and not nx.has_path(G, s1, s2):\n",
    "                return s1, s2\n",
    "\n",
    "\n",
    "class DataLabeler:\n",
    "    @staticmethod\n",
    "    def label(cross_encoder, df):\n",
    "        data = []\n",
    "        with tqdm(total=len(df)) as progressbar:\n",
    "            for idx, row in df.iterrows():\n",
    "                sentences = [row[\"sentence1\"], row[\"sentence2\"]]\n",
    "                data.append(sentences)\n",
    "                progressbar.update()\n",
    "\n",
    "        scores = cross_encoder.predict(data, show_progress_bar=True)\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def create_silver(df, scores):\n",
    "        df[\"label\"] = scores.tolist()\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05e63c9-092c-494e-99b2-fd84d1f3e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sts_dataset = load_dataset(\"stsb_multi_mt\", name=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d2b61b-a4eb-4abf-a8bf-d17705a46fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad77465a82f34179a3f534295a451e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Scores:   0%|          | 0/5749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5dd860124574f5bab001e0a00a96305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Scores:   0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47962980fffb4c599a449c5d8c309d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normalizing Scores:   0%|          | 0/1379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sts_df_train = DatasetProcessor.sts_to_dataframe(sts_dataset[\"train\"])\n",
    "sts_df_dev = DatasetProcessor.sts_to_dataframe(sts_dataset[\"dev\"])\n",
    "sts_df_test = DatasetProcessor.sts_to_dataframe(sts_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9e103-a3dc-4456-a1ce-15cbc45c0c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb7f8f069c34e85a70655ca6d69d70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b1ceab31ed4890bd9e3fee854d1548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639a7d8a0a6a4191a375e3adc31ed09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sts_train = DatasetProcessor.create_sbert_dataset(sts_df_train) # The gold dataset\n",
    "sts_test = DatasetProcessor.create_sbert_dataset(sts_df_dev)\n",
    "sts_dev = DatasetProcessor.create_sbert_dataset(sts_df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7fa5c-23f8-4b75-b890-384f2abacb12",
   "metadata": {},
   "source": [
    "## Training Cross-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f7a58b-5eee-4178-b88a-a34499c38806",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "\n",
    "train_dataloader = DataLoader(sts_train, shuffle=True, batch_size=batch_size)\n",
    "evaluator = CECorrelationEvaluator.from_input_examples(sts_dev, name=\"dev\")\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)\n",
    "\n",
    "cross_encoder = CrossEncoder(\n",
    "    crossencoder_name, num_labels=1,\n",
    "    default_activation_function=torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c88660dd-2e99-42bd-9967-e6333e4ecbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 21:13:59 - Use pytorch device: mps\n"
     ]
    }
   ],
   "source": [
    "cross_encoder = CrossEncoder(\n",
    "    \"kaggle/working/crossencoder/\", num_labels=1,\n",
    "    default_activation_function=torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d3cb57-0ca0-43f8-857b-3eb16d16b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=100,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=os.path.join(SESSION_PATH, \"crossencoder\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfacaa1-f3d4-4a8e-8c11-257abc68c287",
   "metadata": {},
   "source": [
    "## Create Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e8ebf314-d4d5-4f16-86f2-ef0cddd302a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%capture\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"quora\")\n",
    "dataset = dataset[\"train\"].train_test_split(train_size=30_000, test_size=75_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4658b076-0d46-4db9-a1a9-ec1d32ede162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How are bacteria beneficial to humans?</td>\n",
       "      <td>How can bacteria be harmful to humans?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am turning 30 this week, and I am lost caree...</td>\n",
       "      <td>Why do some people have a tenacious personality?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which is the best site to play indian rummy wi...</td>\n",
       "      <td>Which is the best site to play an Indian rummy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is your most common problem?</td>\n",
       "      <td>What are the most common problems in teaching?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Can we build cars that run on tap water?</td>\n",
       "      <td>Why don't we have cars that run on water?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73214</th>\n",
       "      <td>Yale University: What was it like to attend Ya...</td>\n",
       "      <td>Yale University: What was it like to attend Ya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73215</th>\n",
       "      <td>How is the formula for copper II sulfate hydra...</td>\n",
       "      <td>What is the formula for copper II sulfate?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73216</th>\n",
       "      <td>How do I prove the existence of God to an athe...</td>\n",
       "      <td>How can you prove to someone that GOD exists?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73217</th>\n",
       "      <td>Why has eBay failed?</td>\n",
       "      <td>Why did eBay fail in China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73218</th>\n",
       "      <td>How do l improve my communication skills?</td>\n",
       "      <td>How can I improve my communication effectively?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73219 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence1  \\\n",
       "0                 How are bacteria beneficial to humans?   \n",
       "1      I am turning 30 this week, and I am lost caree...   \n",
       "2      Which is the best site to play indian rummy wi...   \n",
       "3                      What is your most common problem?   \n",
       "4               Can we build cars that run on tap water?   \n",
       "...                                                  ...   \n",
       "73214  Yale University: What was it like to attend Ya...   \n",
       "73215  How is the formula for copper II sulfate hydra...   \n",
       "73216  How do I prove the existence of God to an athe...   \n",
       "73217                               Why has eBay failed?   \n",
       "73218          How do l improve my communication skills?   \n",
       "\n",
       "                                               sentence2  \n",
       "0                 How can bacteria be harmful to humans?  \n",
       "1       Why do some people have a tenacious personality?  \n",
       "2        Which is the best site to play an Indian rummy?  \n",
       "3         What are the most common problems in teaching?  \n",
       "4              Why don't we have cars that run on water?  \n",
       "...                                                  ...  \n",
       "73214  Yale University: What was it like to attend Ya...  \n",
       "73215         What is the formula for copper II sulfate?  \n",
       "73216      How can you prove to someone that GOD exists?  \n",
       "73217                        Why did eBay fail in China?  \n",
       "73218    How can I improve my communication effectively?  \n",
       "\n",
       "[73219 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetProcessor.filter_quora(quora_df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "59e1e1d0-957d-4ea8-a78f-11fc7d853b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dfb9072937145339706c9484dcc26aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71c7d3e632c4f1a89ff0bea5c7cca8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quora_df_train = DatasetProcessor.qoura_to_dataframe(dataset[\"train\"])\n",
    "quora_df_dev = DatasetProcessor.qoura_to_dataframe(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5b273-9b63-44ed-8adf-6fe31bb711ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = DataLabeler.label(cross_encoder, quora_df_train)\n",
    "dev_labels = DataLabeler.label(cross_encoder, quora_df_dev)\n",
    "\n",
    "quora_df_train = DataLabeler.create_silver(quora_df_train, train_labels)\n",
    "quora_df_dev = DataLabeler.create_silver(quora_df_dev, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fad19e3-9a0a-4c48-93bb-c3f04064b64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272819a6c4444b1888acd22867efd44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9cec57cdf04c90bed4b7c25ae313a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_samples_train = DatasetProcessor.negative_sample(quora_df_train, 5000)\n",
    "negative_samples_dev = DatasetProcessor.negative_sample(quora_df_dev, 5000)\n",
    "\n",
    "silver_train = pd.concat([quora_df_train, sts_df_train, negative_samples_train]).reset_index(drop=True)\n",
    "silver_dev = pd.concat([quora_df_dev, sts_df_dev, negative_samples_dev]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d7a465f-753f-4173-8355-e417445c6bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150ba19e59eb41088a1c40493d969aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40749 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32692b85b844ab6adce304897ed69bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "silver_train_sbert = DatasetProcessor.create_sbert_dataset(silver_train)\n",
    "silver_dev_sbert = DatasetProcessor.create_sbert_dataset(silver_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1387da-a518-405a-b5ae-75332a544790",
   "metadata": {},
   "source": [
    "## Training Bi-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648273f9-4063-4454-bc6f-df38ac2d699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "encoder = models.Transformer(biencoder_name, max_seq_length=128)\n",
    "pooling_model = models.Pooling(\n",
    "    encoder.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False\n",
    ")\n",
    "bi_encoder = SentenceTransformer(modules=[encoder, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326cd694-c047-4925-98c0-a6f5b480647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(silver_train, shuffle=True, batch_size=batch_size)\n",
    "train_loss = losses.CosineSimilarityLoss(model=bi_encoder)\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(silver_dev, name=\"dev\")\n",
    "num_epochs = 5\n",
    "\n",
    "warmup_steps = math.ceil(len(silver_train) * num_epochs / batch_size * 0.1)\n",
    "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
    "\n",
    "bi_encoder.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=os.path.join(SESSION_PATH, \"biencoder\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a0bdf-262b-4a13-822b-1f132fb8334f",
   "metadata": {},
   "source": [
    "## Load Fine-Tuned Bi-Encoder & Encode Test Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc7a8a-2865-48e5-be1f-984ae5a6861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(\n",
    "    \"kaggle/working/crossencoder/\", num_labels=1,\n",
    "    default_activation_function=torch.nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a07940-f33d-4034-bf88-1d7a21f19548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "bi_encoder = SentenceTransformer(\"kaggle/working/biencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36fbd243-c2f4-4913-a00c-b3486de0953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_df_dev.to_csv(\"data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de0ea618-8b0d-4ef8-88cb-76f7b94b5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence1 = quora_df_dev[\"sentence1\"].tolist()\n",
    "all_sentence2 = quora_df_dev[\"sentence2\"].tolist()\n",
    "all_sentences = list(set(all_sentence1 + all_sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c875eb0f-d4cd-4468-ae17-216b66967af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924b1ae68c834db5b4a5abe1c47a12c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_sentences_embedding = bi_encoder.encode(all_sentences, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac93faf-d050-4a7b-95a1-fc6daa15aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/embeddings.npy\", all_sentences_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9054e1b5-4955-4913-ad74-dddf32fdca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_embedding = np.load(\"data/embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7159c990-696e-4b2d-a34c-fad2b8f96052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128469, 768)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sentences_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb92b33b-b3f2-4a95-a676-1b7d4d0a8aed",
   "metadata": {},
   "source": [
    "## Approximate Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c8889ef-35ad-4fc7-a328-841bb3330ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(model, sentence, preprocess=True):\n",
    "    return model.encode(sentence, show_progress_bar=False)\n",
    "\n",
    "def faiss_search(model, embeddings, query, k):\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    query_vector = get_sentence_vector(model, query, False)[:, None].T\n",
    "    D, I = index.search(query_vector, k=k)\n",
    "    return I\n",
    "\n",
    "def rerank(I, query, sentences, cross_encoder):\n",
    "    results = [sentences[i] for i in I[0]]\n",
    "    reranking_scores = [cross_encoder.predict([query, doc], show_progress_bar=False) for doc in results]\n",
    "    docs = list(zip(results, reranking_scores, range(len(results))))\n",
    "    docs_sorted = list(sorted(docs, key= lambda x: x[1], reverse=True))\n",
    "    return docs_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "644a4115-8fe9-488d-9c9f-fc38a4295c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How do I delete my account at Quora?', 0.6080411, 1),\n",
       " ('How do I delete Quora account?', 0.5275337, 0),\n",
       " ('How do I get a Quora Account?', 0.4865062, 2),\n",
       " ('How did I get a Quora account?', 0.44115204, 3),\n",
       " ('What does \"Quora\" mean?', 0.15222643, 4)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"i want to trash my quora account\"\n",
    "\n",
    "I = faiss_search(bi_encoder, all_sentences_embedding, query, k=5)\n",
    "rerank(I, query, all_sentences, cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5345fc7-21f3-4cea-b0e4-f99bce04725f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Is there one god or many?', 0.71820015, 0),\n",
       " ('Where is god?', 0.6188236, 1),\n",
       " ('Is there any proof that God exists?', 0.5178925, 4),\n",
       " ('What is a \"god\"?', 0.5027556, 2),\n",
       " ('What is the meaning of one god?', 0.34638438, 3)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"is there any god?\"\n",
    "\n",
    "I = faiss_search(bi_encoder, all_sentences_embedding, query, k=5)\n",
    "rerank(I, query, all_sentences, cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "da3db2ec-8f5a-4ed7-bfd7-a6b7fca7ff52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('How is deep learning used in finance?', 0.71429634, 3),\n",
       " ('Who is the best deep learning teacher?', 0.6813439, 4),\n",
       " ('Is deep learning used in trading?', 0.6724121, 1),\n",
       " ('How does deep residual learning work?', 0.6713976, 0),\n",
       " ('Kevin Murphy: What do you think of Deep Reinforcement Learning?',\n",
       "  0.6100112,\n",
       "  2)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"deep learning\"\n",
    "\n",
    "I = faiss_search(bi_encoder, all_sentences_embedding, query, k=5)\n",
    "rerank(I, query, all_sentences, cross_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b15af6cb-fa56-49df-bdd2-681d67b76efb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What would be the best way to quit smoking?', 0.39267415, 8),\n",
       " ('What are some ways to quit smoking?', 0.35488284, 6),\n",
       " ('How do one quit smoking?', 0.34245422, 5),\n",
       " ('How do I quit smoking and drinking?', 0.340137, 7),\n",
       " ('How do you quit smoking?', 0.33898452, 3),\n",
       " ('how to quit smoking', 0.3258829, 0),\n",
       " ('How do I quit smoking?', 0.3130946, 4),\n",
       " ('How do stop smoking?', 0.3014821, 9),\n",
       " ('?', 0.09699556, 1),\n",
       " ('o', 0.09331581, 2)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"smoking\"\n",
    "\n",
    "I = faiss_search(bi_encoder, all_sentences_embedding, query, k=10)\n",
    "rerank(I, query, all_sentences, cross_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9228717-d56c-48f0-ad35-5b5a0c6dc6aa",
   "metadata": {},
   "source": [
    "## kNN Search with Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a959d101-686b-4ffc-a3a1-87847882ab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49545ed1-0a1a-4f80-b9d5-9105379d9342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/safak.bilici/miniconda3/envs/default/lib/python3.11/site-packages/elasticsearch/connection/http_urllib3.py:209: UserWarning: Connecting to https://localhost:9200 using SSL with verify_certs=False is insecure.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ES_LOCALHOST = \"http://127.0.0.1:9200\"\n",
    "\n",
    "USERNAME = \"elastic\"\n",
    "PASSWORD = \"UJa8*7BwbFaCu*y5lDzE\"\n",
    "\n",
    "client = Elasticsearch(\n",
    "    \"http://localhost:9200\",\n",
    "    http_auth=(USERNAME, PASSWORD), \n",
    "    verify_certs=False, \n",
    "    use_ssl=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0f79d070-b937-49e0-a7cb-4d44d72d7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(sentences, embeddings):\n",
    "    bulk = []\n",
    "    for sentence, embedding in zip(sentences, embeddings):\n",
    "        sample = {}\n",
    "        sample[\"question\"] = sentence\n",
    "        sample[\"question_vector\"] = embedding.tolist()\n",
    "        bulk.append(sample)\n",
    "    return bulk   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c388ea0-0d3a-4623-8b92-3211f54241ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = prepare_dataset(all_sentences, all_sentences_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4c539b5f-04a7-4e13-9b80-d797eb1c5c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 21:53:02 - POST https://localhost:9200/_bulk [status:200 request:6.495s]\n",
      "2024-04-05 21:53:10 - POST https://localhost:9200/_bulk [status:200 request:6.337s]\n",
      "2024-04-05 21:53:19 - POST https://localhost:9200/_bulk [status:200 request:6.937s]\n",
      "2024-04-05 21:53:27 - POST https://localhost:9200/_bulk [status:200 request:6.905s]\n",
      "2024-04-05 21:53:37 - POST https://localhost:9200/_bulk [status:200 request:7.334s]\n",
      "2024-04-05 21:53:46 - POST https://localhost:9200/_bulk [status:200 request:7.530s]\n",
      "2024-04-05 21:53:56 - POST https://localhost:9200/_bulk [status:200 request:7.815s]\n",
      "2024-04-05 21:54:09 - POST https://localhost:9200/_bulk [status:200 request:8.998s]\n",
      "2024-04-05 21:54:17 - POST https://localhost:9200/_bulk [status:200 request:6.006s]\n",
      "2024-04-05 21:54:26 - POST https://localhost:9200/_bulk [status:200 request:6.641s]\n",
      "2024-04-05 21:54:35 - POST https://localhost:9200/_bulk [status:200 request:7.304s]\n",
      "2024-04-05 21:54:44 - POST https://localhost:9200/_bulk [status:200 request:7.278s]\n",
      "2024-04-05 21:54:53 - POST https://localhost:9200/_bulk [status:200 request:7.111s]\n",
      "2024-04-05 21:55:02 - POST https://localhost:9200/_bulk [status:200 request:7.305s]\n",
      "2024-04-05 21:55:11 - POST https://localhost:9200/_bulk [status:200 request:7.310s]\n",
      "2024-04-05 21:55:18 - POST https://localhost:9200/_bulk [status:200 request:5.135s]\n",
      "2024-04-05 21:55:27 - POST https://localhost:9200/_bulk [status:200 request:7.006s]\n",
      "2024-04-05 21:55:37 - POST https://localhost:9200/_bulk [status:200 request:7.839s]\n",
      "2024-04-05 21:55:46 - POST https://localhost:9200/_bulk [status:200 request:6.933s]\n",
      "2024-04-05 21:55:52 - POST https://localhost:9200/_bulk [status:200 request:3.031s]\n"
     ]
    }
   ],
   "source": [
    "def bulk(documents):\n",
    "    for doc in documents:\n",
    "        yield {\n",
    "            \"_index\": \"inzva-index\",\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "\n",
    "\n",
    "success, _ = helpers.bulk(client, bulk(documents), chunk_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7e772ce-8468-4c64-9a77-33e91bafb3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 21:56:12 - PUT https://localhost:9200/inzva-index/_settings [status:200 request:0.375s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_index_settings = {\n",
    "    \"refresh_interval\": \"60s\",\n",
    "    \"number_of_replicas\": 1\n",
    "}\n",
    "client.indices.put_settings(\n",
    "    index=\"inzva-index\",\n",
    "    body={\"settings\": post_index_settings},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4936a7b3-0b42-4167-a2aa-e322f67aaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_embedding(bi_encoder, query):\n",
    "    return bi_encoder.encode(query).tolist()\n",
    "\n",
    "\n",
    "def knn_query(query_embedding, query, boost, num_candidates, size):\n",
    "    return {\n",
    "        \"knn\": {\n",
    "            \"field\": \"question_vector\",\n",
    "            \"query_vector\": query_embedding,\n",
    "            \"k\": size,\n",
    "            \"num_candidates\": num_candidates,\n",
    "            \"boost\": boost\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def multi_match_query(query, boost, size):\n",
    "    return {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "        \"bool\": {\n",
    "          \"should\": [\n",
    "            {\n",
    "              \"multi_match\": {\n",
    "                \"query\": query_text,\n",
    "                \"fields\": [\"question\"],\n",
    "                \"boost\": boost\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "def hybrid_query(query_embedding, query, knn_boost=1, multi_match_boost=1, num_candidates=300, size=15):\n",
    "    query = {}\n",
    "    \n",
    "    knn = knn_query(query_embedding, query, knn_boost, num_candidates, size)\n",
    "    multi_match = multi_match_query(query, multi_match_boost, size)\n",
    "\n",
    "    query.update(knn)\n",
    "    query.update(multi_match)\n",
    "    return query\n",
    "\n",
    "\n",
    "def search(client, query):\n",
    "    output = []\n",
    "    response = client.search(index=\"inzva-index\", body=query)\n",
    "    for sample in response[\"hits\"][\"hits\"]:\n",
    "        output.append((sample[\"_score\"], sample[\"_source\"][\"question\"]))\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b0f871a2-2e0f-4e58-ad3b-e87088ceb671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c386cd0906a4c23a859a12a07ad8ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_text = \"spring boot\"\n",
    "query_embedding = query_to_embedding(bi_encoder, query_text)\n",
    "\n",
    "query = hybrid_query(query_embedding, query, 0.9, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6003581f-f780-4b0b-b522-173a6e01f04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 21:58:22 - POST https://localhost:9200/inzva-index/_search [status:200 request:0.028s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2.593286,\n",
       "  'What is the difference between Spring Boot and the Spring framework?'),\n",
       " (2.1398466,\n",
       "  'Should I learn Spring boot Framework or Spring Framework? What is the difference between them and which one is the best?'),\n",
       " (1.7295177, 'What is boot process?'),\n",
       " (1.6919584, 'What is a boot camp?'),\n",
       " (1.6444197, 'What is a boot loader?'),\n",
       " (1.6259985, 'How do you learn spring framework?'),\n",
       " (1.6195064, 'How do I learn Spring Framework?'),\n",
       " (1.5687168, 'How do I learn Spring Framework? Help?'),\n",
       " (1.5658484, 'Where can I get spring framework videos?'),\n",
       " (1.5274035, 'How is still water different from spring water?'),\n",
       " (1.4190416, 'How do I boot from SSD with clone of Windows 10?'),\n",
       " (1.0340872, 'Is the Arab Spring over?'),\n",
       " (1.0282699, 'Coding boot camps Bay area?'),\n",
       " (0.9876111, 'What exactly is the Arab Spring?'),\n",
       " (0.9876111, 'How good is Poland Spring water?')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9158433-5936-4fd9-bcef-683270167304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100f5f2123524ff29eeade897a776e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_text = \"huawei\"\n",
    "query_embedding = query_to_embedding(bi_encoder, query_text)\n",
    "\n",
    "query = hybrid_query(query_embedding, query, 0.9, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59b34c8a-7c65-4414-9b53-f063c68dd089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-05 21:59:09 - POST https://localhost:9200/inzva-index/_search [status:200 request:0.095s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.6322346, 'Are there any functional huawei service centers in Gurgaon?'),\n",
       " (1.0528336, 'Iphone 6 plus vs Huawei p9 plus?'),\n",
       " (1.0094175, 'Which Huawei Switches Support Static Multicast Router Ports?'),\n",
       " (1.0094175, 'Did my Huawei y 360-431 sappoting vidiocall?'),\n",
       " (1.0094175, 'What is difference LTE Huawei and LTE alcatel?'),\n",
       " (0.9694406, 'Where can I find Huawei service center in Gurgaon?'),\n",
       " (0.9694406, 'For what does Huawei Mate 9 need four microphones?'),\n",
       " (0.9325094, 'When will the Huawei Honor 4X get the Lollipop update?'),\n",
       " (0.89828885, 'How do I turn off the screen overlay in Huawei P8lite?'),\n",
       " (0.86649084, 'Which is the best custom rom for the Huawei Honor 3c 4G?'),\n",
       " (0.8368672,\n",
       "  'How is the after sales service of Xiaomi, Huawei and Gionee in India?'),\n",
       " (0.8092021,\n",
       "  'Should I update my Huawei Honor 7 to Marshmallow from Lollipop. Why? Why not?'),\n",
       " (0.7590188,\n",
       "  'Have Huawei Mate 9 Porsche design launched on the market? How/where can I reserve it?'),\n",
       " (0.6571369,\n",
       "  'How do I invite engineers to share configuration or troubleshooting cases about Huawei Enterprise product, such as switches, AR, WLAN, Controller?'),\n",
       " (0.63995683,\n",
       "  'Should I buy a LG k10 or a Huawei p8 lite? Cause right now only these 2 phones are within my budget.')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(client, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a58db5-4df9-4899-97aa-a0741db03b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
